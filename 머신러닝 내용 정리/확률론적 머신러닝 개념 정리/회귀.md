

# 회귀

  회귀 : 입력과 출력과의 관계를 찾아내어 모델링하는것. 데이터의 관계를 알아내는 것이 회귀

  분류도 회귀. 분류는 사진이나 특징만 보고 어떤 분류에 해당하는지 예측하는 것.
   분류는 딱딱 떨어지지만, 회귀는 실수값, 즉 연속적인 값.  
# 선형회귀

  선형회귀 : 입력과 출력의 관계를 선형이라고 가정하고 그 관계를 잘 설명하는 직선을 찾는 것.
   하지만 현실적으론 직선관계인 입력과 출력의 관계는 매우 드뭄.
    그렇기에 입력과 출력의 관계가 실제로 선형이 아니더라도, Parameter와 입력값, 출력값을 선형결합으로 나타낼 수 있으면 선형회귀.
  입력과 출력의 관계의 모양은 상관없이 Parameter에 대해 선형인 함수로 관계를 표현해 데이터를 잘 설명하는 모델을 알아내는 것.
   입력값을 변수가 아닌 상수로 보고, parameter값을 변수로 본 상태에서 선형인 방정식이면 선형회귀.

 ex) 입력값과 출력값의 관계가 2차함수형식이라고 할때, Parameter는 $w_1, b$가 있다고 가정.
  그러면 다음과 같은 그래프가 그려짐.

<figure>
<img src ="img/선형회귀(2차함수).png">
</figure>

위와 같이 입력값과 출력값의 관계가 비선형이지만, parameter$[w_1, w_2, b]$에서 보면 선형.
 $x^2 \ x$를 변수가 아닌 상수로 일반적인 숫자로 보고, $w_1, w_2, b$를 변수로 보면 Parameter에 대해 선형인 함수로 볼 수 있음.
$x^2 = z_1, x = z$로 변환해서 식을 다시 쓰면

$y = w_1z_1 + w_2z+ b$ 와 같은 형식의 선형방정식으로 정의가 가능.

 즉 ,입력과 출력간의 관계가 아닌 , Parameter 입장에서 봤을 때의 선형결합인 것이 선형회귀.


   
 입력을 x 출력을 y라 할때 그래프의 모양이 2차방정식과 같은 비선형이어도 선형회귀 일수 있음

 <figure>
 <img src="img/선형회귀(비선형그래프).png">
 </figure>

  위와 같이 그래프의 관계를 곡선으로 표현한 것처럼 보여도 선형회귀를 사용한 모델일 수 있음.
  즉, 선형회귀는 데이터와의 관계를 설명하는 '직선'을 찾는 다는 것이 아니라, Parameter에 대해 선형인 함수를 의미함.


   즉, 데이터를 잘 설명하는 '직선'이 아닌 '선'을 찾는것. (직선일 수도 있고, 위처럼 곡선일 수도 있음)
 
  
  목적 : 처음보는 입력에 대해서도 적절한 출력을 얻기 위함. => 입력만 보고도 출력을 예측하기 위함

  선형으로 가정한 것이기에 데이터간의 관계가 실제론 비선형적일수도 있음.
   선형회귀라는것은 weight가 선형적인 함수임.

   입력과 출력의 관계를 선형(ax+b)형식으로 놓고 a,b를 알아내서 처음보는 입력에도 출력을 예측.

## 선형회귀의 LOSS

   LOSS는 얼마나 모델이 Label을 예측하지 못했는가에 대한 척도.
   선형회귀에선 데이터를 설명하는 직선과 Label(정답)과의 거리를 의미.

  $w_1x+b$  라면 x = 1일때에는 예측값은 $w_1+b$가 됨. 이것과 실제 x=1일때의 정답을 비교한 것을 Error라고 가정.

  즉 실제 정답 y - 예측값 $\hat y$로 표현이 가능. 그렇다면 저 Error값을 다 더해서 Loss로 쓰면 되나?
  정답은 아님.
  아래의 경우를 예시로 들 수 가 있음

<figure>
<img src = "img/선형회귀 아이스크림 산점도 LOSS.png">
</figure>



$x = [15.2, 17.1, 23 ,25.8, 30.8]$
$y = [88, 100 ,128, 155 ,182]$ 인 상태,
위는 최고기온이 다음과 같을때 아이스크림의 총 판매량을 나타낸 산점도 그래프

선형회귀를 사용해 저렇게 선을 기울기와 y절편을 정했다고 가정.
이때의 Error값은 총 5개가 나옴

 e1 = 88 - 89.2, e2 = 100 - 100.6, e3 = 128-136, e4 = 155-152.8, e5 = 182-182.8

Loss = e1+e2+e3+e4+e5라고 하면 Loss= -1.2-0.6-8+2.2-0.8 = -8.4로 Loss가 0 미만이 되어 버림

이와 같이 그냥 더해버리면 Loss가 0미만으로 가거나 아니면 좋지 않은 직선이어도 Loss가 0에 가까워져 버리는 경우가 생김.

그렇기에 error값들은 양수로 변환이 필요.
error값을 제곱하거나, 절대값을 씌우고 합하면 Loss가 됨.





## Ridge Regression

^cbafe1

  선형 회귀의 확장버전.
  과적합 방지를 위해 선형회귀에 [[MAP (maximum a posterior estimation)#^46cd43| L2 정규화]]를 추가한 기법임.

 선형회귀에선 데이터가 적거나 feature가 많을때 weight가 과도하게 커져서 훈련데이터에만 과적합하는 문제가 있음
  그래서 L2 정규화를 사용함

정의:

 기본 선형회귀 Loss 함수 : $MSE = \frac{1}{N}\sum_{n=1}^{N}(y_n - w^T x_n)^2$

 Ridge 함수 추가 : $L(w)= \frac{1}{N}\sum_{n=1}^{N}(y_n - w^T x_n)^2 + \lambda{\left\lVert w\right\rVert}^{2}_{2}$ 


 가중치 벡터 w를 축소하는 형태.
shrink와 달리 weight 벡터 전체를 0벡터 쪽으로 균일하게 끌어당기는 방식


   