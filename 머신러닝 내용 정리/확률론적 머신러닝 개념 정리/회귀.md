

# 회귀

  회귀 : 입력과 출력과의 관계를 알아내는것. 데이터의 관계를 알아내는 것이 회귀

  분류도 회귀. 분류는 사진이나 특징만 보고 어떤 분류에 해당하는지 예측하는 것.
   분류는 딱딱 떨어지지만, 회귀는 실수값, 즉 연속적인 값.  
# 선형회귀

  선형회귀 : 입력과 출력의 관계를 선형으로 가정하고 데이터를 잘 나타내는 직선을 알아내는 것.목적 : 처음보는 입력에 대해서도 적절한 출력을 얻기 위함. => 입력만 보고도 출력을 예측하기 위함

  선형으로 가정한 것이기에 데이터간의 관계가 실제론 비선형적일수도 있음.
   선형회귀라는것은 weight가 선형적인 함수임.

   입력과 출력의 관계를 선형(ax+b)형식으로 놓고 a,b를 알아내서 처음보는 입력에도 출력을 예측.

## 선형회귀의 LOSS

   LOSS는 얼마나 모델이 Label을 예측하지 못했는가에 대한 척도.
   선형회귀에선 데이터를 설명하는 직선과 Label(정답)과의 거리를 의미.

  $w_1x+b$  라면 x = 1일때에는 예측값은 $w_1+b$가 됨. 이것과 실제 x=1일때의 정답을 비교한 것을 Error라고 가정.

  즉 실제 정답 y - 예측값 $\hat y$로 표현이 가능. 그렇다면 저 Error값을 다 더해서 Loss로 쓰면 되나?
  정답은 아님.
  아래의 경우를 예시로 들 수 가 있음

<figure>
<img src = "img/선형회귀 아이스크림 산점도 LOSS.png">
</figure>



$x = [15.2, 17.1, 23 ,25.8, 30.8]$
$y = [88, 100 ,128, 155 ,182]$ 인 상태,
위는 최고기온이 다음과 같을때 아이스크림의 총 판매량을 나타낸 산점도 그래프

선형회귀를 사용해 저렇게 선을 기울기와 y절편을 정했다고 가정.
이때의 Error값은 총 5개가 나옴

 e1 = 88 - 89.2, e2 = 100 - 100.6, e3 = 128-136, e4 = 155-152.8, e5 = 182-182.8

Loss = e1+e2+e3+e4+e5라고 하면 Loss= -1.2-0.6-8+2.2-0.8 = -8.4로 Loss가 0 미만이 되어 버림

이와 같이 그냥 더해버리면 Loss가 0미만으로 가거나 아니면 좋지 않은 직선이어도 Loss가 0에 가까워져 버리는 경우가 생김.

그렇기에 error값들은 양수로 변환이 필요.
error값을 제곱하거나, 절대값을 씌우고 합하면 Loss가 됨.
wmr



## Ridge Regression

^cbafe1

  선형 회귀의 확장버전.
  과적합 방지를 위해 선형회귀에 [[MAP (maximum a posterior estimation)#^46cd43| L2 정규화]]를 추가한 기법임.

 선형회귀에선 데이터가 적거나 feature가 많을때 weight가 과도하게 커져서 훈련데이터에만 과적합하는 문제가 있음
  그래서 L2 정규화를 사용함

정의:

 기본 선형회귀 Loss 함수 : $MSE = \frac{1}{N}\sum_{n=1}^{N}(y_n - w^T x_n)^2$

 Ridge 함수 추가 : $L(w)= \frac{1}{N}\sum_{n=1}^{N}(y_n - w^T x_n)^2 + \lambda{\left\lVert w\right\rVert}^{2}_{2}$ 


 가중치 벡터 w를 축소하는 형태.
shrink와 달리 weight 벡터 전체를 0벡터 쪽으로 균일하게 끌어당기는 방식


   