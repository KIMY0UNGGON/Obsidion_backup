<hr>

## 빈도주의 통계

 베이지안과 달리 Parameter(w,b)를 고정적인 불확실 상수로 취급.
 데이터셋만 랜덤으로 가정.

 반복된 시도로 인해 발생하는 변화를 사용. 즉, 데이터 셋으로 학습을 함.

추정은 MLE를 사용한 점추정을 사용.

추정량 : $\hat{\Theta}$로 표현되며 딥러닝에서는 학습 알고리즘을 의미함. 

$\theta^*$ : 정답 파라미터로, 우리가 구하고 싶은 파라미터. 실제로는 우리가 모르므로, 데이터로 추정하여 추측함.

$\hat\theta$ : 우리가 추정한 추측 파라미터. 이 추정값이 틀릴 수 있으므로, 부트스트랩이나 가우시안 근사로 불확실성을 측정함. 학습이 끝난 모델의 가중치


### 샘플링 분포


 빈도주의적 통계에서 [[용어정리|불확실성]]을 사후 분포가 아닌 추정량의 샘플링 분포로 표현됨.

 파라미터(weight, bias)를 고정된 값으로 보고, 데이터 셋을 랜덤 변수로 취급해서 사용.
 데이터 셋을 랜덤하게 반복적으로 뽑아서 모델을 재학습시켜서 나타낸 추정값을 분포로 나타냄.

##### MLE의 샘플링 분포의 가우시안 근사

  우리가 사용하는 가장 흔한 추정은 MLE

  샘플링분포를 직접 계산이 불가능에 가까움. 그렇기에 샘플크기가 커지면 모델에서 MLE의 샘플링 분포가 가우시안 모양이 된다는 점을 이용해 근사.
  즉, 샘플이 크면 쉽게 근사가 가능.

 
[[피셔 정보 행렬 FIM|FIM]] 정의 -> $F(\theta) \triangleq \mathbb{E}_{x \sim p(x|\theta)}[\nabla log\ p(x|\theta)(\nabla log\ p(x|\theta))^T]$

 FIM은 Log Likelihood의 기울기의 공분산으로 정의됨.
 FIM의 역할은 분산을 정밀하게 조절하는 것.

샘플링분포의 분산 및 공분산은$((N\times F(\theta^*))^{-1}) =$ $\frac{1}{N \times F(\theta)}$ 로 근사됨

  FIM은 원래 하나의 데이터 포인트가  파라미터를 얼마나 정확히 표시하는지 알려주는 척도.
  N은 샘플의 개수 즉, 데이터의 양을, FIM은 데이터의 질을 반영해 불확실성을 결정.

 샘플링 분포가 좁을 수록 Var이 작음. 즉, 데이터가 많고, 데이터가 파라미터를 잘 나타내게 함.
 즉 분산이 작으면 파라미터가 정답에 가깝다는 것을 의미.

 공분산은 가우시안 분포에서 퍼짐의 방향과 폭을 결정.
 평균이 중심위치를 결정하고, 공분산이 얼마나 어떤 방향으로 퍼질지를 조절함.
 


샘플링 분포는 다음과 같이 정의 됨 $\text{SamplingDist}(\hat{\Theta}_{\text{mle}}, \theta^*) \to \mathcal{N}(\cdot \mid \theta^*, (N F(\theta^*))^{-1})$

##### 부트스트랩 근사: 임의 추정량의 샘플링 분포

 부트스트랩 -> 몬테카를로 근사 기법의 일종.

 추정량이 데이터의 복잡한 함수이거나, 데이터의 수가 부족할때 추정량을 근사(현재 추정이 맞는지 안맞는지를 알게)하기 위해 부트스트랩을 사용함.
 
 현재 가지고 있는 데이터셋을 가지고 복원 추출을 하여 가짜 데이터 셋을 여러개 만들어 근사.
 즉, 현재 데이터 셋에서 $[x_1\ , y_1]$이  이미 나와도 다시 데이터셋에 넣고 랜덤하게 추출하여 가짜 데이터셋을 여러개 만들어서 사용함.

 이렇게 만든 데이터셋으로 현재 파라미터를 사용하여 지금 파라미터가 얼마나 정확한지를 나타냄.


 분포의 분산이 작으면 정밀도가 높음.
 즉, 분산이 작거나, 근사한 샘플링 분포가 가우시안 모양이면 정밀도가 높음.

 정밀도가 높다는 건 해당 파라미터를 신뢰할만 하다는 뜻으로, 진짜 $\theta^*$ 즉, 정답에 가깝다는 뜻은 아님.


<figure>
<img src = "img/부트스트랩.png">
</figure>


위의 사진을 보면, (b)와 (d)는 분산이 작고(폭이 좁고) 가우시안 모양에 가까워, 정밀도가 높음
(a), (b)

##### 부트스트랩은 가난한 자의 사후분포다

 사후분포와 부트스트랩을 계산된 샘플링 분포는 사전붙포가 강하지 않은 경우 상당히 유사할 수 있음.

 

 
### 신뢰구간
  
  통계에서 표본평균으로 모평균을 알아낼때 95% 신뢰구간을 자주 사용해서 95%를 사용.
  표준정규분포에서 95%의 신뢰구간의 구간은 |1.96|

  머신러닝에서 신뢰구간은 우리가 찾는 값, 머신러닝에선 정답을 잘 예측하는 파라미터,를 데이터 셋으로 무수히 많이 반복했을 때 95%는 찾는 값이 들어있다.

 만약에 100번 반복을 했으면 95개는 우리가 찾는 Parameter를 찾는다는 의미.

 이런 신뢰구간을 찾는 방법은 2가지가 존재 : 가우시안 근사, 부트스트랩

**가우시안 근사**

  데이터가 충분하게 많으면 분포가 정규분포로 근사하는 원리를 이용한 근사.
  즉, 데이터가 충분하면 에러값에 대한 분포가 정규분포로 근사할 것이라고 가정해서 문제를 품.

 신뢰구간 근사 공식을 사용.

 p( |$\mu$| < 1.96 $\times\ \frac{\sigma}{\sqrt{n}}$ ) 

 장점 : 계산이 빠름.
 단점 : 데이터가 적어, 정규분포에 근사하지 못해 분포가 이상해지면 신뢰구간이 틀림.
  

#### 부트스트랩

 현재 가지고 있는 데이터 셋으로 샘플링해서 가상의 데이터셋을 생성해 분포를 만듦.
 이렇게 만들어진 데이터셋의 분포를 가지고 신뢰구간을 정의 하는게 부트스트랩.

 


#### 신뢰 구간 != 신용구간

 위에서 말한 95%의 신뢰 구간은 정답 파라미터가 95% 확률로 저 구간들 안에 있다는 게 아님.
 위에서의 95%는 이 방법을 100번 반복하면 95번은 진짜 파라미터를 포함하는 구간을 만든다는 의미. 

 신뢰구간은 '얼마나 잘 했는가?'를 말하고, 신용구간은 $\theta$가 어디 있는가는 말해줌


### 편향-분산 트레이드오프(The bias-variance tradeoff)

 몰라
#### 편향

 평균적으로 얼마나 틀리는가를 의미함.
  수식의 정의는 다음과 같음.


 $bias(\hat\theta) \triangleq \mathbb{E}[\hat\theta(D)] - \theta^*$

 반복 실험시에, 현재 파라미터가 정답과 얼마나 벗어나는가를 의미.
 bias가 0이면 비편향이라고 말함.

 딥러닝에서 비편향이라는 것은 데이터 셋에 과적합 했다는 의미기도 함.


