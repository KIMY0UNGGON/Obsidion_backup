
#### - 베르누이 분포(정규분포)

가장 단순한 확률 분포.

$0 ≤ X ≤ 1$ 의 확률을 가지는 분포.

대표적인 **예**로 **동전 던지기**가 있음.

Y= 1이 동전의 앞면, Y=0이 동전의 뒷면을 나타낸다고 가정.

$P(1) = y,\ P(0) = 1-P(1) = 1-y$가 됨.

이를 $Y∼Ber(y)$로 표현. (~기호는 Y에서 표본이 추출된다 또는 Y로 분포된다는 의미)

베르누이 분포는 이산확률변수를 가지므로 pmf로 표현됨.

pmf는 다음과 같음.
$Ber(Y) = \begin{cases} 1-y & \text(Y=0일때) \\ y & \text(Y=1일때) \end{cases}$


이것을 간결하게 표현하면 $Ber(Y) = y^Y(1-y)^{1-Y}$

분포의 그림은 다음과 같음.

<figure>
<img src = "정규분포.png">
<figurecaption>정규분포 그래프</figurecaption>
</figure>






#### Categorical 분포

K개의 카테고리가 있다고 했을 때 그 카테고리 중 하나를 선택하는 것.

ex) 주사위 던지기는 k=6인 카테고리 시행

베르누이는 k=2인 카테고리 시행이라고도 볼 수 있음.

주사위 던지기는 (x=1, x=2, x=3, x=4, x=5, x=6) - 주사위 카테고리 시행의 확률변수.

각 카테고리가 선택될 확률은 $p = (p_1, p_2, p_3, p_4, p_5, p_6)$ $p_{1-6} = 1/6$

one-hot-encoding :

categorical 데이터를 수치형 데이터로 변환하는 기법. categorical 데이터가 숫자가 아닐때 이를 모델이 이해할 수 있는, 숫자의 형태로 바꾸기 위해 사용됨.

즉, categorical 데이터의 각 고유 값을 이진 벡터 형식으로 표현하는 방법. categorical 데이터는 하나의 열로 표현되며, 해당 categorical에 해당하면 1, 그렇지 않으면 0으로 표현됨.

one-hot인 이유는 한 categorical만 “hot(1)”이기 때문.

one-hot-encoding에서 열은 K개를 가짐. 주사위 던지기에 열은 6개.

주사위 던지기 one-hot-encoding ⇒ (1,0,0,0,0,0) , (0,1,0,0,0,0), (0,0,1,0,0,0), (0,0,0,1,0,0), (0,0,0,0,1,0),(0,0,0,0,0,1)





 
#### 경험적 분포(Empirical distribution):

실제 데이터 샘플을 가지고 확률 분포를 근사하는 방법.
즉, 주어진 데이터 셋을 가지고 만든 확률 분포.
 ex) 주사위를 예시로 들면 1,2,3,4,4,4,5 로 경험적 분포를 만들면 실제분포와는 달라지지만 비슷한 분포로 근사가능

확률 분포 p(X)에서 추출된 N개의 데이터를 가지고 있을때 그 데이터들을 가지고 원래의 분포 특성을 추정하는것.

non-parametric한 방법. 분포의 형태를 미리 가정하지 않고, 데이터 자체를 기반으로 분포를 나타냄.

empirical pdf

$\hat{p}_N(x) = \frac{1}{N} \sum_{n=1}^N \delta_{x^{(n)}}(x)$


 [[디랙 델타 함수(Dirac Delta)]] : $\delta_{x^{(n)}}(x)$ ⇒ $x = x^{(n)}$일때 무한대의 값을 그 외에는 0의 값을 가짐.

 $\frac{1}{N}$은 정규화하는 역할을 함.



스파이크 : 경험적 pdf에서 데이터 샘플이 존재하는 위치에 확률 밀도가 집중된 것을 시각적으로 묘사한 것.

데이터 셋이 {2,4,4,6} 일때, N = 4

$\hat{p}_N(x) = \frac{1}{4} \sum_{n=1}^4 \delta_{x^{(n)}}(x) = \frac{1}{4}(\delta_2(x)+\delta_4(x)+\delta_4(x)+\delta_6(x))$

x축은 샘플 값 {2,4,6} y축은 각 위치에서의 확률밀도, 즉 스파이크 높이를 나타냄.

<figure>
<img src="Empirical1.png" >
<figcaption>경험적 분포 pdf</figcaption>
</figure>

경험적 밀도 함수는 데이터 샘플이 존재하는 위치만 “스파이크” 형태로 값이 나타남. 다른 곳은 0으로 표시.

실제 분포 p(X)가 연속적이어도 이산적인 스파이크 형태로 나타남.

샘플 수가 많아 질수록 경험적 pdf는 실제 분포에 가까워짐 ⇒ 큰수의 법칙에 의함.

empirical cdf

$\hat{P}_N(x) = \frac{1}{N} \sum_{n=1}^N I(x^{(n)} \leq x) = \frac{1}{N} \sum_{n=1}^N u_{x^{(n)}}(x)$

$u_y(x)={\begin{cases} 1 & \text{if } x \geq y \\ 0 & \text{if } x < y \end{cases} }​$

$u_y(x)$는 계단 함수.

$I$는 지시함수.

N = 5 이고 $x^{(1)} = 1, x^{(2)} = 2, x^{(3)} = 2, x^{(4)} = 3, x^{(5)} = 4$ 일때

$x < 1$ 일때는 $\hat{P}_N(x)$ = 0

$x = 1$ 일때는 $\hat{P}_N(x) = \frac{1}{5}$

$x = 2$ 일때는 $\hat{P}_N(x) = \frac{3}{5} => x^{(3)}$ 도 2이기때문에 총 3개의 샘플

$x = 3$ 일때는 $\hat{P}_N(x) = \frac{4}{5}$

$x = 4$일때 $\hat{P}_N(x) = \frac{5}{5}$

<figure>
<img src="Empirical2.png">
<figurecaption>경험적 분포 cdf</figurecaption>
</figure>

경험적 cdf는 비감소 함수. x가 증가함에 따라 0에서 1로 올라감

샘플이 없는 구간에서는 값이 일정, 샘플 위치에서 계단 처럼 증가.

샘플 수가 많아질수록 경험적cdf도 원래의 cdf와 가까워짐.




#### Laplace 분포

<figure><img src = "Laplace.png"><figurecaption>라플라스 분포 그림</figurecaption></figure>


#### 베타분포
 구간 [0,1] 에 대한 지지를 가지는 분포. 지지 =>  확률이 0보다 큰 모든 결과들의 집합.
  지지란, 실제로 일어날 가능성이 있는 모든 사건의 범위를 뜻함.
  확률들의 확률 분포라고 할 수 있음.

 베타분포는 두개의 하이퍼 파라미터 a와 b를 가짐.
   a: 내가 지금까지 성공이라고 본 횟수. (성공이라 믿는 정보의 양)
   b: 내가 지금까지 실패라고 본 횟수. (실패라 믿는 정보의 양) 
   이 하이퍼 파라미터들은 pseudo-count(유사 카운트)라고 부름
    a= b= 1이면 성공도 실패도 본적 없는 완전 무지의 상태 이때는 Uniform형태가 됨.


pdf 정의 => $Beta(x|a,b) = \frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1} = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}$

베타분포의 pdf에는 베타 함수 $B(a,b)$를 사용.
 여기서 베타함수는 정규화 상수로서 사용됨.
 $B(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$ 

베타 함수에선 감마함수 $\Gamma(a)$를 사용함. 

$\Gamma(a) = \int_0^{\infty}x^{a-1}d^{-x}dx$ 


평균은 $\frac{a}{a+b}$  a를 성공횟수, b를 실패 횟수로 봐서 a+b를 총 시도 횟수로 봄.
   a= b = 2면 2/(2+2)로 0.5가 됨

최빈값 = $\frac{a-1}{a+b-2}$ pdf에서 최대가 되는 지점을 찾음. pdf에 log씌워 미분

분산 = $\frac{ab}{(a+b)^2(a+b+1)}$


  중요한 이유:
    확률 자체의 불확실성을 모델링하는데 탁월함.
    정의역이 [0,1]이라 이 구간의 값들에 대한 확률 분포를 나타낼 수 있음
     동일한 정의역을 지닌 확률, 비율, 성공률들의 확률분포를 표현 가능하다는 뜻
    사전믿음(prior belief)을 수학적으로 표현하는 도구로 사용됨. 확률 변수에 대한 확률분포를 나타낸다는 뜻.
    


베르누이 시행의 확률을 모델링하는 모든곳에 사용가능.
베타분포는 미지의 확률 값에 대한 우리의 믿음의 분포.

#### - 분포의 기댓값:
[[확률론적 머신러닝 - 확률#^bf7439]]


#### 혼합 분포(mixture distribution)


분포의 볼록 조합을 취하는 것 => 혼합 분포

$p(y|\theta) = \sum_{k=1}^K\ W_kp_k$

W는 혼합 가중치(mixture weight)를 의미. 어떤 모델을 더 신뢰하고, 어떤 모델을 더 무시할지를 혼합 가중치로 정함.

$p_k$는 k번째 혼합요소 즉, k번째의 분포를 의미

잠재변수(latent variable) h를 선언. h는 현재, 몇번째 가중치와 몇번째 분포인지를 나타냄

p(h=k|$\theta$) = $\pi_k$
$p(y|h=k,\theta) = p(y|\theta_k) = p_k(y)$ 

$p(y|\theta) = \sum_{k=1}^{K}p(h=k|\theta)p(y|h=k,\theta) = \sum_{k=1}^{K}\pi_k p(y|\theta_k)$ 

##### 가우시안 혼합 모델(GMM, Gausian Mixture Model)

가우시안 모델의 혼합

$p(y|\theta) = \sum_{k=1}^{K}\pi_kN(y|\mu_k,\Sigma_k)$




##### 베르누이 혼합 모델(BMM, Bernoulli Mixture Model))

$p(y|z=k,\theta) =  \prod_{d=1}^{D}Ber(y_d|\mu_dk) = \prod_{d=1}^{D}\mu_{dk}^{y_d}(1-\mu_{dk})^{1-y_d}$


지수의 덧셈을 위해 GMM과 달리 곱셈으로 표현
