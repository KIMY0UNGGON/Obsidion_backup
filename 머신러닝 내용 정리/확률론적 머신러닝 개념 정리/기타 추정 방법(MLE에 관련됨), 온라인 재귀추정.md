#### 1. 모멘트(Method of Moments, MOM) - 적률법이라고도 함
 [[MLE]]를 계산하기 위해선 $∇θNLL(θ)=0$을 풀어야함.
 이때 간단하게 MOM으로 접근할 수 있음.
 
 MOM은 확률 분포의 Parameter(ex: 평균, 분산)을 추정하는 방법.
 이론적 모멘트(분포의 기대값)와 경험적 모멘트(데이터에서 계산된 기대값)을 동일시해서 Parameter를 구함.
 모멘트는 1~k차 까지 존재. $k = {1,2,...,K}$ K는 Parameter의 개수
 데이터는 ${\{y_1,y_2,...,y_N\}}$까지 존재

 - 이론적 모멘트 : 분포의 기대값
 -$\mu_k = \mathbb{E}[Y^k]$ 로 정의됨.
 -1차 모멘트는 $\mu$ , 2차 모멘트는 $E[Y^2]$식으로 증가함.
 
 - 경험적 모멘트 : 실제 데이터의 기대값
 -$\hat{\mu}_k = \frac{1}{N} \sum_{n=1}^N y_n^k$ 로 정의됨​

이때 $\mu_k$ == $\hat\mu_k$ 라고 가정하여서 연립방정식으로 만들고, parameter를 추정함.

- 예시: 단변량 가우시안에서의 MOM
-가우시안 분포는 Parameter가 평균과 분산 2가지가 존재 K = 2
-K=2이므로 $\mu_1,\mu_2,\hat\mu_1,\hat\mu_2$를 각각 구해 연립방정식을 취하면 됨.
-$\mu_1 = \hat\mu_1 (4.69)$  parameter중 하나인 평균은 다음과 같이 구할 수 있음
-$\mu_2=\hat\mu_2 = \sigma^2 + \mu^2$ => 데이터 제곱합의 평균은 $E[X^2]-\mu^2 = \sigma^2 => E[X^2] = \mu^2+\sigma^2$  로 정의가능.
-이때 우리는 나머지 parameter인 분산을 구해야 하므로 식을 정리하면
$\sigma^2 = \hat\mu_2 - \mu^2$ 가 됨


#### 2. Online (recursive) estimation - 온라인 재귀 추정:

Batch Learning(배치 학습) : 데이터셋 전체가 학습 시작전에 사용가능 한 학습.
**Online Learning** 
	데이터가 한꺼번에 주어지지 않고, 시간에 따라 순차적으로 도착하는 상황에서 모델을 학습시키는 방법. 실시간 데이터를 다룰때 유용함.
	새로운 데이터가 도착할 때 마다 모델 parameter를 빠르게 업데이트해 일정한 시간 복잡도를 유지하는 것이 핵심. 추정치는 아래 식과 같이 계산됨
	$\theta_t = f(\hat{\theta}_{t-1}, y_t)$ => 재귀적 업데이트. 이전 추정치와  새로운 데이터 만으로 계산됨.

가우시안 분포에서의 재귀적 MLE:
	$\hat{\mu}_t = \frac{1}{t} \sum_{n=1}^t y_n$ 다음과 같이 가우시안에서는 예측 평균이 계산됨
	이때 이전 추정치 $\hat\mu_{t-1}$ 과 새로운 input $y_n$ 만을 가지고 다음 추정치를 계산하려고 함.
	 $\hat\mu_{t-1} = \frac{1}{t-1}\Sigma_{n=1}^{t-1}y_n   =>   (t-1)\hat\mu_{t-1} = \Sigma_{n=1}^{t-1}y_n$ 로 풀이가 가능.
	 즉, $\hat\mu_t = \frac{1}{t}((t-1)\hat\mu_{t-1}+y_n)$ 으로 변경이 가능. 
	 이걸 다시 풀면 $\hat\mu_t = \hat\mu_{t-1}+\frac{1}{t}(y_n - \hat\mu_{t-1})$ 이 됨. 이 식을 이동평균(moving average)라 함.
	 $\frac{1}{t}$ 는 보정의 크기를 조절하는 weight. 데이터가 많아 질수록 새로운 데이터의 영향이 점차 줄어듬.
	 이동평균 특징
		보정항 : $y_t-\hat\mu_{t-1}$ 식은 새로운 데이터가 기존 추정치와 얼마나 다른지를 나타냄.
				이 차이를 $\frac{1}{t}$ 로 스케일링하여 업데이트
		시간에 따른 안정화 : $\frac{1}{t}$의 영향으로 데이터가 들어올 때마다 새로운 데이터의 영향이 줄어듬.
		효율성 : 전체 데이터 저장의 필요가 없음. 직전 추정치와 데이터 수, 새로운 데이터 만으로 다음 추정치를 계산이 가능.

##### 지수 가중 [[이동평균]](EWMA) :
   EWMA는 지수 이동평균(EMA)라고도 불림.
   일반적인 이동평균과 달리 최근 데이터에 더 많은 가중치를 주고, 과거의 데이터들은 지수적으로 가중치를 줄이는 방식. => 과거의 데이터들의 영향을 줄여나감.

 정의:
	$\hat\mu_t = \beta\hat\mu_{t-1}+(1-\beta)y_t$
	$\hat\mu_t$는 시간 t에서의 이동평균 추정값 => 즉, 현재 최근에 업데이트한 추정값.
	$\hat\mu_{t-1}$은 이전 시간 t-1에서의 이동평균.
	$y_t$는 현재 새롭게 입력된 데이터
	$\beta$는 과거의 데이터의 기억을 얼마나 오래 유지할지 결정하는 감쇠계수 
	 $\beta$ 는 지수이동평균의 1 - 평활계수 
	 머신러닝에선 하이퍼파라미터. 0 < $\beta$ < 1
데이터가 많을 수록 가중치의 합은 1에 가까워짐
 $\hat\mu_t = \beta\ \mu_{t-1} + (1-\beta)y_t$  
 $\hat{\mu}_t = (1 - \beta) y_t + (1 - \beta) \beta y_{t-1} + (1 - \beta) \beta^2 y_{t-2} + \cdots + (1 - \beta) \beta^{t-1} y_1 +(1 - \beta) \beta^t y_0$ 으로 표현가능.
 이때 기하급수의 합으로 가중치의 합은 $(1-\beta^{t+1})$로 수렴. 데이터가 입력될 수록 
 t->$\infty$ 에 가까워지므로  $\beta$는 0에 수렴해서 가중치의 합은 1에 수렴.

초기 바이어스 보정:
$\tilde{\mu}_t = \frac{\hat{\mu}_t}{1 - \beta^t}$ 로 정의됨. 초기에는 $(1-\beta)^t$의 값이 1보다 작아 $\hat\mu_t$가 실제 평균보다 작게 추정됨.
그래서 정규화해 바이어스를 줄임.

장점:
	최근 데이터에 민감.
	계산이 간단하고 효율적임 => 재귀적 계산을 사용하기 때문.
	메모리 사용량이 적음. 
	과거 데이터를 모두 저장하지 않고 데이터 수 = t와 이전 이동평균($\hat\mu_{t-1}$)과 새로운 입력값만 있으면 다음 이동평균을 추정가능.
단점:
	초기 바이어스 문제로 보정이 필요.
	$\beta$값을 어떻게 정하냐에 따라 결과가 달라짐.
	Outlier에 민감할 수 있음.