새로운 정보를 바탕으로 기존의 확률을 합리적으로 갱신하는 방법.

   조건부 확률의 전환.
	조건부확률은 어떤 원인이 주어졌을때 결과가 나타날 확률을 구하는 것.
	베이즈 정리는 그 반대로 어떤 결과가 관측되었을때, 그게 특정 원인일 확률을 구하는것.
	└ P(결과|원인) 으로 P(원인|결과)를 추론하는게 베이즈 규칙

   공식 정의 => $P(원인|결과) = \frac{P(결과|원인)P(원인)}{P(결과)}$
	P(원인|결과) => 사후확률
	└ 최종적으로 구하고 싶은값. ex) 양성 반응이 나왔을때(결과), 실제로 걸렸을(원인) 확률 
	P(결과|원인) => LikeliHood
	└ 원인이 사실일때 결과가 관측될 확률. ex) 실제 병에 걸린(원인)사람을 검사했을때 양성반응(결과)가 나올 확률. (검사의 민감도)
	P(원인) => 사전확률
	└결과를 관측하기 전에 원인에 대한 초기 믿음(확률)
	P(결과) => 증거확률
	└결과 자체가 관측될 확률. 사후 확률의 총합이 1이 되도록 만들어 주는 정규화 상수

다음과 같은 구조로 표현 가능 최종 믿음 (Posterior) ∝ (Prior) × (Likelihood)

머신러닝에선 모델의 parameter를 고정된 값이 아닌 확률 분포로 간주.
 데이터가 주어지기 전 parameter에 대한 사전확률을 설정, 데이터를 관찰하면서 이 사전확률을 갱신하여 사후확률 분포를 얻음. => 얻은 결과로 모델의 불확실성 측정 가능.

 사전확률 갱신 => 데이터를 본 후에 초기 추측을 수정하여 더 나은 추측으로 나아감.
 P(A|B)와 P(B)로 P(B|A)를 구하는 것이라 보면됨