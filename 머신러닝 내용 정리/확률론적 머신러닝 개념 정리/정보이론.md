

## 정보량

 정보량 => I(x)는 당연한 확률(확률값이 1에 가까운 확률)보다 일어날 가능성이 적은 일이 더 가치가 높음

ex) 해가 동쪽에서 뜬다(모두가 알고 있고, 무조건 일어나는 사건이므로 정보량의 가치가 0)
   서울에서 7.8규모의 지진이 일어난다(매우 일어날 확률이 낮음 정보량의 가치가 높음)


그래서 정보량의 경우 다음과 같은 모양을 가짐

<figure>
<img src= "img/negative_log_graph.png">
</figure>


x축이 확률이고, y축이 정보량.

즉 정보량 I(x) = $-log_2(p(X))$ 밑항이 2인이유는 bit를 사용하기 때문





## 엔트로피 - 정보량의 기댓값(평균)


H(x) 로 표현

$H(x) = E[I(x)] = \sum_{x\in X}p(x)I(x) = -\sum_{x \in X}p(x)log_2 p(x)$


예시 ) 베르누이 분포의 동전던지기
  P(앞) = 0.5, P(뒷) = 0.5라고 가정. X = {앞면, 뒷면}.
  H(x) = $-\sum_{x \in X}p(x)log_2 p(x) = -(0.5)log_2 0.5 -(0.5)log_2 0.5 = (0.5)+(0.5) = 1$

  동전던지기의 경우 평균 1bit의 정보가 필요하다는 의미

 엔트로피가 높으면 :
  결과 예측이 어려움( 불확실성이 높음)
  모든 사건이 일어날 가능성이 거의 비슷(위의 동전던지기가 그 예시)
  평균 정보량이 많음

엔트로피가 낮을 때:
  결과 예측이 쉬움(불확실성이 낮음)
   특정 사건이 매우 높게 나타남



## KL-Divergence(Kull back-Leibler Divergence)

한국어론 쿨백 발산

두개의 모델(분포)간의 차이를 측정하는 측도
KL-Divergence = 0이면 두개의 분포가 같음


머신러닝에선 라벨 분포와 학습 모델간의 차이를 나타냄. 측 KL Divergence가 최소면 모델과 라벨이 일치.
 그래서 학습 목표는 KL-Divergence가 0에 가깝게하는 것이 목표.

$D_{KL}(P||Q) = \sum_x P(X)log(\frac{P(X)}{Q(X))})$


 특징:
   비대칭성 : P와 Q가 일치하지 않을 때는 $D_{KL}(P||Q) != D_{KL}(Q||P)$이다. 앞에 있는 분포를 기준분포로 비교하는 것이기 때문.
   
   P = Q이면 $D_{KL}(P||Q) = 0$으로 일치함




## Cross-Entropy

 정답(라벨) 분포 P를 기준으로 모델이 예측한 Q를 사용해 정보를 압축(인코딩)할때 평균적으로 필요한 정보량을 나타냄

 즉, 내가 현재 학습시킨 모델과 정답이 얼마나 다른지를 알기 위한 척도로 사용된다

식 정의
 $H(P, Q) = -\sum_xp(x)log_2Q(x)$

 일반적인 엔트로피 $H(P)$와 달리 내가 학습시킨 모델의 정보량을 weight로 사용하여 가중평균을 구함

KL-발산과 엔트로피의 합으로도 표현 가능

$D_{KL}(P||Q) = \sum_x P(X)log(\frac{P(X)}{Q(X))}) = \sum_xP(X)log_2P(X) -\sum_xP(X)log_2Q(X) = H(P, Q) - H(P)$

이를 이용하면 $H(P,Q) = D_{KL}(P||Q) + H(P)$로 표현가능


