

기초 내용 [[Bayesian Rule(베이시안 규칙, 정리 기타등등) | 베이즈 규칙]]


### 1. Conjugate Priors(공액 사전)

 사후 분포를 계산할 수 있는 사전확률과 Likelihood를 고려.
  likelihood에 conjugate인 사전분포를 사용하는 경우.

 사전 분포를 $p(\theta) \in F$라 할때 사후 분포 p($\theta|D$) $\in F$다.
  즉 사전분포와 사후 분포가 같은 분포의 형태를 가진다는 뜻이다.
  F는 분포의 가족(지수족)를 의미( ex. 가우시안, 베르누이, 카테고리 등)




### 2. 베타 이항모델


 동전을 N번 던져서 앞면의 확률을 추론할때 $y_n = 1$ 은 n번째 시도가 앞면이었다고 나타내고, $y_n = 0$은 n번째 시도가 뒷면이었다고 나타냄

 이는 $D = \{y_n : n = 1 : N\}$으로 나타내도 D는 모든 데이터를 의미
 이를 $y_n ~ Ber(\theta)$ 로 가정함. 베타 이항모델은 $p(\theta|D)$를 계산하는 방법을 설명함.

#### 베르누이 Likelihood

 데이터는 iid(모두 독립이고 같은분포의 형태를 띈다)라고 가정함.
그래서 베르누이 likelihood는 곱셈으로 정의


$p(D|\theta) = \prod_{n=1}^{N}\theta^{y_n}(1-\theta)^{1-y_n} = \theta^{N_1}(1-\theta)^{N_0}$

$N_1 = \sum^N_{n=1} I(y_n=1)$ 즉, 앞면인 경우의 총합
$N_0 = \sum_{n=1}^{N} I(y_n=0)$ 동전이 뒷면인 경우의 총합
$N = N_1 + N_0$


#### Binomial Likelihood( 이항 Likelihood )

Likelihood : $P(D|\theta) = Bin(y|N,\theta) = \binom{N}{y} \theta^y (1 - \theta)^{N-y}$
$\binom{N}{y}$는 $\theta$와 관련이 없으므로 무시가능


#### 사전(prior)

  베타 이항모델에서의 사전분포는 켤레(conjugate prior)이라 가정
  즉, $p(\theta) \in Beta$로 사전분포가 베타분포의 형태를 띔
$p(\theta) \propto \theta^{\tilde{\alpha} - 1} (1 - \theta)^{\tilde{\beta} - 1} \propto \text{Beta}(\theta | \tilde{\alpha}, \tilde{\beta})$

#### 사후분포

사후분포는 베르누이 Likelihood $\times$ 사전분포 / $p(D)$

즉, 다음과 같이 정의됨 
Posterior =  $\theta^{\tilde{\alpha} - 1} (1 - \theta)^{\tilde{\beta} - 1}\times\theta^{N_1}(1-\theta)^{N_0} = \theta^{\tilde{\alpha} - 1+N_1} (1-\theta)^{N_0+\tilde{\beta} - 1}$
 $\hat\alpha$ = $\tilde\alpha+ N_1$, $\hat\beta = \tilde\beta+N_0$라 가정하면 posterior = $\theta^{\hat\alpha} (1 - \theta)^{\hat{\beta}}$

#### 사후 최빈값(Posterior Mode)

 $\hat{\theta}_\text{MAP} = \arg\max_\theta p(\theta|D) = argmax_\theta\ (log\ p(\theta) + log\ p(D|\theta))$
 

#### 사후 평균(Posterior Mean)

 사후 최빈값은 사후분포의 요약으로 부족할 수 있음
 사전 강도와 MLE의 볼록 결합


 mean = $\int p(\theta|D) d\theta$  = E$[\theta|D]$ = $\frac{\hat\alpha}{\hat\alpha+\hat\beta}$ = $\frac{\hat\alpha}{\hat N}$
 $E[\theta|D] = \frac{\tilde{\alpha} + N_1}{\tilde{\alpha} + N_1 + \tilde{\beta} + N_0} = \frac{\tilde{N} m + N_1}{N + \tilde{N}} = \frac{\tilde{N}}{N + \tilde{N}} m + \frac{N}{N + \tilde{N}} \frac{N_1}{N} = \lambda m + (1 - \lambda) \hat{\theta}_\text{MLE}$

사전 강도가 약할 수록 $\lambda$가 작아져 사후평균이 MLE에 가까워짐


#### 사후분산

 추정치의 불확실성 측정을 위한 분산.
 추정치의 표준오차를 계산하는것.


 se($\theta$) = $\sqrt {V[\theta|D]}$ 

  $V[\theta|D] = \frac{\hat\alpha\hat\beta}{(\hat\alpha+\hat\beta)^2(\hat\alpha+\hat\beta+1)}$ 
  
$V[\theta|D] \approx \frac{N_1 N_0}{N^3} = \frac{\hat{\theta} (1 - \hat{\theta})}{N}$


$\sigma = \sqrt{V[\theta|D]} \approx \sqrt{\frac{\hat\theta(1-\hat\theta)}{N}}$

불확실성(se)은 $\frac{1}{\sqrt{N}}$비율로 감소함. 분산이 $\hat\theta = 0.5$일때 최대가 되고, 0이나 1에 가까울때 최소
1또는 0이면 분산이 0이 되기 때문

#### 사후예측(posterior Predictive)

 훈련된 데이터로 사후 예측을 함. 이때 한쪽에 치우쳐져(과적합되어) 있을 수 있으므로 MAP추정으로 계산.


Plug-in Approximation : 훈련 데이터로 $\theta$를 계산

 이때 과적합이 발생할 수 있음. 
   ex) 동전을 3번 던졌을 때 3번 모두 앞면이 나오면 뒷면이 나올 확률이 무시되는 과적합현상이 발생

 MAP으로 추정하여 사전확률의 강도를 줌(뒷면이 나올수도 있다라는 것을 인식. 과적합 방지)

 fully Bayesian을 사용하여 parameter를 점추정하지 않고, 전체 분포를 사용해 예측
 $\theta$를 점이 아닌 분포 $p(\theta|D)$로 취급하여 평균을 구해 새로운 데이터 값을 예측

$p(y_new|D) = E[\theta|D]$

 즉 fully Bayesian은 예측을 한가지의 값이 아닌 균형이 맞도록 범위(분포)로 정해 과적합을 방지.


#### 베르누이 모델의 사후 예측

  베르누이 모델 같은 경우 사후 예측 분포가 다음과 같음

$p(y=1 | D) = \int_{0}^{1}p(y=1|\theta)p(\theta)|D)d\theta = E[\theta|D] = \frac{\hat\alpha}{\hat\alpha+\hat\beta}$


 add one smoothing을 beta(1,1)을 사용해도 얻을 수 있음.

$p(y =1 |D) = \frac{N_1 + 1}{N_1+N_0 + 2}$


#### 이항 모델의 사후예측


Fully Bayesian
$p(y|D, M) = \int_0^1 \text{Bin}(y|M, \theta) \text{Beta}(\theta | \hat{\alpha}, \hat{\beta}) d\theta = \binom{M}{y} \frac{1}{B(\hat{\alpha}, \hat{\beta})} \int_0^1 \theta^y (1 - \theta)^{M-y} \theta^{\hat{\alpha}-1} (1 - \theta)^{\hat{\beta}-1} d\theta$
 
  $\int_0^1 \theta^{y + \hat{\alpha} - 1} (1 - \theta)^{M - y + \hat{\beta} - 1} d\theta = B(y + \hat{\alpha}, M - y + \hat{\beta})$


$\text{Bb}(x|M, \hat{\alpha}, \hat{\beta}) \triangleq \binom{M}{x} \frac{B(x + \hat{\alpha}, M - x + \hat{\beta})}{B(\hat{\alpha}, \hat{\beta})}$

 Fully Bayesian은 예측을 점이 아닌 분포로 취급
 이 분포의 평균을 사후예측 값으로 사용.
 분포의 평균을 값으로 사용해서 적은 데이터에선 플러그인 근사보다 조금 더 부드러운 예측이 가능

현재 데이터가 Head = 3, tail = 0이고 사전분포가 Beta(2,2)이면 사후 분포는 Beta(3+2,2)로 플러그인 보단 조금 더 부드러움


플러그인 근사

$p(\theta|D) \approx \delta(\theta - \hat{\theta})$
$p(y|D, M) = \int_0^1 \text{Bin}(y|M, \theta) p(\theta|D) d\theta = \text{Bin}(y|M, \hat{\theta})$

플러그인 근사는 간단하지만 불확실성을 무시함. overfit 문제가 있음
플러그인도 MAP로 사전확률을 사용해 예측 하지만 unseen 이벤트에서 현재 나오지 않은 이벤트의 확률이 극단적으로 작다는 단점이 있음. 그 때문에 적은 데이터에선 overfit(과적합) 위험이 큼

현재 데이터가 Head = 3, tail = 0이고 Beta(2,2)이면 

 $\hat\theta_{mle} = \frac{3 + 2 -1}{3+ 2 + 2 -2} = \frac{4}{5} = 0.8$
 

플러그인 근사는 한개의 값을 예측.


데이터가 적을 땐 Fully Bayesian을 많은 때엔 플러그인 근사가 좋음



#### 주변 Likelihood(Marginal Likelihood)

 Marginal Likelihood : 데이터 D가 매개변수없이 사전 분포만으로 나올 확률
 
 모델 M에 대한 주변 Likelihood 또는 증거(evidence)는 다음과 같이 정의됨

$p(D|M) = \int p(\theta|M) p(D|\theta, M) d\theta$

또는 $p(D|h =k) = \int p(D|\theta)p(\theta|h=k)d\theta$

$p(D|\theta)$는 Likelihood
$p(\theta|h=k)$는 조건부 사전확률
적분으로 분포의 평균을 냄

$\theta$를 적분으로 없애서 모델 전체의 증거 역할을 함

 보통의 경우 $\theta$항과는 무관하여 무시됨
 하지만 모델의 비교와 하이퍼 파라미터 튜닝의 핵심도구 역할을 함

 주변 Likelihood를 모델의 데이터 설명도로 사용 : 모델 비교시
 
 주변 Likelihood가 높으면 fit이 좋고 overfit 가능성 존재
   낮으면 underfit 가능성 존재


 Empirical Bayes Prior를 데이터로 튜닝


 ex) 상황: 웹사이트 클릭 D (N=100 trials, 20 clicks)
      Hyperparam 후보: (α,β)=(1,1) uniform vs (2,2) weak fair.
      - p(D|(1,1)) = \binom{100}{20} B(21,81)/B(1,1) ≈ 0.0001 (계산).
	  - p(D|(2,2)) ≈ 0.00008 (작음).
	  P(D|(1,1)) 이 더 높으므로 1,1 하이퍼파라미터 선택



 베르누이에서 Marginal Likelihood


#### 혼합 켤레분포( mixtures of conjugate priors)

 현실에선 사전분포가 단일 형태를 띄지 않는 경우도 존재. (공정하다고 생각하기도 하고, 편향되어 있다고 생각도 할 수 있기 때문)
 그래서 사전분포를 mixture 분포 형태로 혼합하여 표현.


ex)

카지노에서 동전던지기를 할때, 동전이 앞면 뒷면 두가지가 같은 확률로 나온다고 생각도 하지만, 카지노에서 동전에 장난질을 쳐놨다고 생각할 수 있음. 이 경우 하나의 베타 분포로 표현이 불가능.
multimodal을 표현못하기에 혼합 분포 사용

 사전분포 $p(\theta)$ = $0.5\ Beta(20,20)$ + $0.5\ Beta(30,10)$ 

 위에선 동전이 공정하다고 생각하는게 0.5, 앞면이 더 잘나올것이라 생각하는 편향이 0.5라고 가정한 상태

h 를 이산잠재변수라 가정하면 다음과 같이 식이 정의 됨
 h는 현재 몇번쨰 분포와 가중치인지를 가리킴

$p(\theta) = \sum_kp(h=k)p(\theta|h=k)$

 전확률 공식으로 $\sum_kp(h=k,\theta) = p(\theta)$가 성립
 $p(h=k)$는 prior mixing weight(사전 혼합 가중치)

  
후방 혼합

$p(\theta|D) = \sum_kp(h=k|D)p(\theta|D, h=k)$
$p(h=k|D)$는 weight로 후방 weight
$p(\theta|D, h=k)$는 조건부 후방.
$p(D|h=k)$는 k를 위한 주변 Likelihood

사전분포가 저렇게 주어지고 앞면이 20번 뒷면이 10번 나온 상황이면

$P(\theta|D) = 0.346Beta(\theta|20+20,20+10) + 0.654Beta(\theta|30+20, 10+10)$

w1 = 0.5 $\times p(D|h=1)$
w2 = 0.5 $\times p(D|h=2)$

p(D|h=1) => marginal Likelihood를 계산해서 업데이트


사전 혼합 가중치(prior mixing weights):

 초기의 사전 가중치- 현재 데이터셋의 데이터를 보지 못한 상태.
 사전을 p(\theta)라고 가정

 $p(\theta) = \sum_k p(h=k)p(\theta|h=k)$
 
 지금은 현재 아직 데이터셋이 없어서, 현재 예시에선, 조작된 동전과, 평범한 동전의 차이를 각각 0.5씩 설정
 식은 Marginal Likelihood * prior weight

 

 
후방 혼합 가중치(posterior Mixing Weights)

 데이터셋의 데이터를 보고 prior을 업데이트한 weight
 
 marginal Likelihood로 업데이트


 P(D | h = 1) => 현재 공정한 동전이라 가정
 p(D | h = 2) => 조작된 동전이라고 가정


  $p(D | h= 1) =  \binom{30}{20}\frac{B(40,30)}{B(20,20)}$

  $p(D | h= 2) = \binom{30}{20}\frac{B(50,20}{B(30,10)}$

  현재 입력 데이터에, 공정한 사전 분포를 더해 분포를 계산. 분모로 공정한 사전분포를 사용해, 가설인 사전분포가 잘 맞는지 확인
  h=1은, 공정하다고 생각한 가설, h=2는 조작된 동전이라고 생각한 가설


 이 Marginal Likelihood에 가설의 사전 weight  0.5를 더해 가설의 비중을 업데이트

 k=1 => $0.04365 \times 0.5 = 0.021825$
 k=2 => $0.08259 \times 0.5 = 0.041295$

 그후 이 가중치를 정규화.
 정규화할 분모는 k=1과 k=2의 가설 weight의 합 => 0.021825 + 0.041295 = 0.06312

후방가중치 $p(h=1 | D)$ 와 $p(h = 2 | D)$로 업데이트

 $p(h=1|D)$ = 0.021825/0.06312 = 0.346
 $p(h=2|D)$ = 0.041295/0.06312 = 0.654


결과적으로 후방분포 $p(\theta|D)$는 저 후방분포와 후방가중치의 곱들의 합으로 이루어짐

$p(\theta|D) = 0.346Beta(40,30) + 0.654 Beta(50,20)$

이 후방분포는 동전이 앞면일 확률 자체의 확률 분포. 즉, 예상값, 출력값들의 확률분포





 2진 확률을 k진 확률로 일반화

 베르누이 -> 디리클레 다항 모델

ex) 동전(2진) -> 주사위(6진)


 하나의 시행에서 K개의 결과중 1개만을 선택하는 확률 모델


디리클레 다항모델의 Likelihood:

 $Y~Cat(\theta)$의 Likelihood

 $p(D|\theta) = \Pi_{n=1}^{N}Cat(y_n|\theta) = \Pi_{n=1}^N\Pi_{c=1}^C\theta_c^{I(y_n=c)} = \Pi_{c=1}^C \theta_c^{N_c}$

 $N_c = \sum_n I(y_n = c)$로 데이터에서 카테고리 C가 나타난 횟수.
 
  결국 하나의 $\theta_c$에서만 지시함수가 1이므로 $\Pi_{c=1}^C\theta_c^{I(y_n=c)}$ 에서 \theta_{yc}만 남음
  그래서 축약하면 전체 Likelihood는 다음과 같아짐

 $p(D|\theta) = \Pi_{n=1}^N\theta_{yc}$


 결과적으로 $p(D|\theta) = \theta_1^{N_1}\theta_2^{N_2}\...\theta_C^{N_C}$
 


 $N_c$는 데이터에서 카테고리 c가 나타난 횟수
 ex) 주사위를 100번 던져서 1이 20번, 2가 30번이면 $N_1$ = 20, $N_2$ = 30이 됨

Prior

 이 사전의 합. $\sum_k \theta_k = 1, \theta_k >= 0$
 카테고리 분포의 conjugate 사전 분포는 디리클레 분포. 디리클레 분포는 다변량 베타 분포

 디리클레 분포는 확률 심플렉스(probablity simplex)에서 정의됨.
 

 -  확률 심플렉스(probablity simplex)
   $\mathbb{S}_K = \left\{ \theta : 0 \leq \theta_k \leq 1, \sum \theta_k = 1 \right\}$

  K = 2: 선분 (0,1) ~ (1,0) 사이
  K = 3: 정삼각형 (모서리:(1,0,0),(0,1,0),(0,0,1))
  K = 4는 정사면체 내부

   모든 확률 벡터 $\theta$는 심플렉스 위에만 존재


 

Posterior
 MAP 추정 사용

 
 사후분포의 parameter 업테이트 규직.


  사후분포는 사전 가상 관측수 + 실제 관측수로 구함

 사후 평균 :

 $\hat\theta_k = \frac{\hat\alpha_k}{\sum_{k'}\hat\alpha_{k'}} = \frac{\tilde\alpha+N_k}{\sum_{k'}(\tilde\alpha_{k'}(\tilde\alpha_{k'}+N_{k'})}$

 N : 실제 데이터 수
 $\alpha_0$: 사전의 강도
 $\frac{N_k}{N}$ : 데이터 빈도 (얼마나 그 데이터가 나타났는지)
 $\frac{\tilde\alpha_k}{\alpha_0}$ : 사전 평균


 스무딩 효과 :
   데이터 N이 많을수록 -> 데이터 비중 올라감 -> MLE에 수렴
   데이터 N이 적으면 -> 사전비중이 올라감 -> 과적합 방지

 - 스무딩 효과 -> 데이터가 없는 경우 0이 아닌 작은 확률을 부여해 추정값이 극단값에 치우치는 것을 방지하는 기법
       사전에 모든 카테고리가 조금씩 관측된 것처럼 가정


 
3. 사후 최빈값 (Posterior Mode = MAP)
$$\tilde{\theta}_k = \frac{\hat{\alpha}_k - 1}{\sum_{k'} (\hat{\alpha}_{k'} - 1)} = \frac{\tilde{\alpha}_k + N_k - 1}{\sum (\tilde{\alpha}_{k'} + N_{k'} - 1)}$$

디리클레 분포의 최빈값(mode)은 항상 $ \alpha_k - 1 $ 기반.
$ \tilde{\alpha}_k = 1 $이면:
$$\tilde{\theta}_k = \frac{N_k}{N} \quad \Rightarrow \text{MAP = MLE}$$



4. 균등 사전일 때: MAP = MLE
$$\tilde{\alpha}_k = 1 \quad \forall k \quad \Rightarrow \quad \text{Dir}(1,1,\dots,1)$$

이 경우:
$$\hat{\alpha}_k = 1 + N_k, \quad \sum \hat{\alpha}_{k'} = K + N$$
$$\text{MAP} = \frac{N_k}{N} = \text{MLE}$$


사후 예측 분포

 - 과거 데이터를 보고 새로운 데이터가 나올 확률을 예측하는 분포

 사전 $\theta$를 고정된 값이 아닌 분포로 생각.
 
  사후 예측 분포 = "훈련 데이터 + 가상으로 관측한 주사위"에서 나올 확률




Marginal Likelihood

 $\theta$ 없이 데이터만으로 계산되는 확률


### 가우시안 가우시안 모델

사전 사후 Likelihood가 모두 가우시안 모델인 켤레 사후 분포
보통은 켤레 사전분포를 따라 사전분포와 사후 분포만 같은데, 가우시안 가우시안은 Likelihood까지 가우시안 모델.

### 켤레 사전분포 너머

 지금까지의 켤레사전분포는 지수족에 속함.
 하지만 현실의 대부분의 모델에선 켤레인 사전분포가 거의 없음.


	모델     사전
 로지스틱        시그모이드    -> 지수족 아님
 포아송 회귀     로그링크      -> 비선형
 혼합 가우시안   혼합           -> 복잡
 신경망             없음

  
#### 비정보적 사전 분포

   데이터에 대한 지식이 거의 없거나 아예 없을때 사용하는 사전분포
   데이터가 스스로 말하게 하라는 원칙을 따름
	   사전이 추정에 거의 영향을 주지 않도록 하는 사전분포
	   아무것도 모르므로 Parameter에 대해 가능한 중립적인 입장을 표현하려는 시도

 비정보적 사전분포를 정의하는 고유한 방법은 없음.



#### 계층적 사전분포(Hierarchical priors)

   베이지안 모델은 Parameter에 대한 사전분포를 지정해야함

  사전분포의 매개변수 -> HyperParameter 즉, 사전분포는 우리가 정해야하는 HyperParameter.
  
  사전분포의 parameter를 $\xi$(싸이)라고 함.
  즉, $\xi$로 $\theta$를 추론하고, $\theta$로 $D$를 추론하는 다단계형식의 계층적 모델.

 계층적 베이지안 모델 또는 다단계 모델이라고 정의함.

#### 경험적 사전분포(Empirical priors)

 MAP, 즉, Posterior 추론의 계산이 편리해지는 근사방법을 설명.
 간단히 말해 사전분포의 HyperParameter를 근사, 추정하는것.

 이때 Type Ⅱ  maximum Likelihood를 사용해 HyperParameter를 추정.

##### Type Ⅱ  maximum Likelihood
 
   데이터를 사용해 베이지안 모델의 하이퍼파라미터를 최적화.
   일반 MLE와 달리 Parameter가 아닌 사전분포의 Hyperparameter를 찾는데 중점을 둠

  데이터 D가 가장 잘 나타나는 하이퍼파라미터를 찾는것.
  


### 베이지안 신뢰 구간(Credible Intervals)

  신뢰구간 : 고차원의 분포, 사후분포를 요약하는데 사용하는데 사용.

  사후 확률이 (1-$\alpha$)인 구간. 이때 $\alpha$는 HyperParameter (보통0.05,0.1 0.01등을 사용)
  
  즉, 데이터를 본 뒤에, Parameter가 특정 구간안에 있을 사후 확률이 (1-$\alpha$)인 구간을 의미. 


#### 머신러닝에서의 의미

 머신러닝에서 보면, Parameter와 예측값도 분포를 가진다고 생각해서 사용함.

 **파라미터 불확실성**
   예시 -> weight 가 [1.8, 2.2] 사이에 있을 확률이 95% → **사후 분포 $p(w \mid D)$** 의 95% **신뢰 구간**

 **예측 불확실성**
  "입력 $x^*$에 대해 예측값 $\hat{y}$가 [10, 12] 사이일 확률이 95%" → **예측 분포 $p(y^* \mid x^*, D)$** 의 95% **신뢰 구간**

 Weight가 특정 구간에 있을 확률이 95% , 입력값이 x일때 예측값이 어느 구간에 있을 확률이 95%일때 여기서 말한 특정 구간이 신뢰 구간.

**실무**
  예측을 얼마나 확신할 수 있는지를 정량화할떄 사용.
   의료분야, 자율주행, 금융과 같이 위험도가 중요한 분야에서 사용함.

베이지안에선 Weight와 예측값 모두 확률분포로 생각.
 Weight의 사후 분포에서 95% 면적을 포함하는 구간 추출 -> weight의 95% 신뢰 구간
 예측값의 사후 분포에서 95% 면적을 포함하는 구간 추출 -> 예측값의 95% 신뢰 구간



 



### 베이즈 머신러닝

 
 지도학습 머신러닝에선 조건부 모델 $p(y|x, \theta)$를 사용.
 Parameter에 대한 사후 분포는 $p(\theta|D)$, 여기서 D는 $D = \{(x_n,y_n) : n = 1, \cdots , N\}$
 더욱 간단히 D를 표현하면 $D = \{(x_1,y_1),(x_2,y_2), \cdots , (x_n,y_n)\}$ 과 같은 훈련데이터.
  x는 입력값, y는 정답 라벨이 된다.

  모델 파라미터를 베이지안 방식으로 추론 => 베이지안 머신러닝.
  즉, 파라미터를 값이 아닌 분포로 보고 추론하겠다.


#### 플러그인 근사(Plugin Approximation)

  Parameter의 사후분포를 주변화.


 Parameter의 사후분포 : 데이터를 본 뒤에 모델이 파라미터에 대해 어떤 믿음을 가지는지 알려줌.
 주변화(Marginalize)는 그 믿음에 따라 실제 예측을 내리는 것.
   사전분포 $p(\theta) + 관측한 데이터\ D$가 합쳐진것이 모델이 파라미터가 어떤 값인지 얼마나 확신하는지를 나타내는 확률분포.

 $\theta$는 베이지안 머신러닝에선 분포로 봄. 그래서 훈련이 아닌 실제 예측에선 $\theta$를 사용이 불가능함. $\theta$는 저장은 되어 있지만 훈련 데이터에만 의존하는 근사 분포.
 
 테스트시에는 $\theta$가 근사 분포이기 때문에 불완전한 분포. 그래서 테스트 시점에선 Marginalize를 사용하여 $\theta$를 제거.

 테스트 시에는 $\theta$를 쓰는것이 아닌 $\theta$의 분포에서 뽑은 정보(평균, 분산, 표준편차 등)로 예측
 $\theta$는 버리고 $\theta$가 계산한 정보들만 남겨서 예측을 함.



식 정의:

 $p(y|x, D) = \int p(y|x, \theta)p(\theta|D) d\theta$ 

위의 식을 사용해 예측분포를 계산을 하지만, 현실적으로 이 적분은 대부분 계산이 불가능
그렇기에 직접 계산이 아닌 근사를 사용.


가장 단순한 근사로는 MLE와 같은 점추정을 사용.
하나의 최적 파라미터 $\hat\theta$ 이 있다고 가정을 한다.

식으론 다음과 같음

 $p(\theta|D) = \delta(\theta-\hat \theta)$ 

 이렇게 사후분포 식을 근사로 구해서 적분이 가능하도록 함.

$\delta(\theta-\hat\theta)$의 경우 sifting property로 적분이 단순하게 가능.

 $p(y|x, D) = \int p(y|x, \theta)\delta(\theta-\hat\theta)d\theta = p(y|x,\hat\theta)$

  위와 같이 적분이 가능해진다.
  


 ##### **예제 스칼라 입력, 이진 출력**

 
   이진분류 사용시, 베르누이 분포에 시그모이드 함수(Activation function)을 사용해서 주로 표현한다.

 시그모이드(로지스틱 함수)는 입력값 실수 $\mathbb{R}$를 [0,1] 구간으로 매핑.
 즉, 정리하면 보통의 이진분류 모델은 로지스틱 회귀를 사용.  

 예시로 붓꽃을 판단하는 이진 분류 모델을 설계한다고 가정해보자.
붓꽃이 Setosa 종인지, Versicolor 종인지 판단하는 분류 모델이다.

 이때 입력은 꽃받침의 길이만 사용한다고 가정해보자.


그러면 1차원 로지스틱 회귀 모델은 다음 형태가 된다.
 $p(y=1|x;\theta) = \sigma(b+wx)$   $\theta$는 Parameter로 여기선 $w\ , b$가 된다.


이 모델에 MLE를 사용해서 적합할때, 플러그인 근사를 사용하면 다음과 같아진다.

$p(y=1|x, \hat\theta)$  $\hat\theta$는 MLE로 찾아낸 Loss가 최저인 Parameter다.
그렇게 되면 다음과 같은 분류모델 형태가 된다.

<figure>
<img src="img/로지스틱붓꽃.png">
</figure>
 
시그모이드를 사용해서 분류형태가 S자 형태로 되어있는 것을 알 수 있다.
이때의 붓꽃이 Setosa인지 Versicolor인지 파악하는 결정 경계는 $\hat y$의 값이 0.5되는, 
 $p(y=1|x^*;\theta)$ = 0.5가 되는 입력값 $x^*$로 정의 된다.
즉, 입력값이 $x^*$가 될때 그 확률값을 결정경계로 사용한다.

위의 그림에서 x= 5.5가 될때 0.5가 되므로, 결정경계를 정하는 입력값은 약 5.5cm인것을 알 수 있다.

이렇게 MLE를 사용해 플러그인 근사를 하면, 빠르게 분류가 가능해지지만, 불확실성이 무시되거나 , 데이터가 적거나, 데이터셋에 노이즈가 많으면$x^*$의 값이 너무 과적합될 수 있다.
즉, MLE의 단점을 그대로 가지고 있다.


베이지안 접근을 사용하면 불확실성을 모델링할 수 있다.


$\theta^s$ ~ $p(\theta|D)$ , $\theta^s$를 사전분포  $p(\theta|D)$에서 랜덤하게 추출했다고 가정했을때 다음과 같이 근사할 수 있다.

$p(y=1|x,D) \approx \frac{1}{S}\sum_{s=1}^S p(y=1|x,\theta^s)$

 Parameter값은 사전분포에서 랜덤하게 가져와서 사용해, 무수히 많은 확률은 낸 다음, 평균을 내서 기대값을 구함.


<figure>
<img src = img/베이지안로지스틱.png>
</figure>

##### *이진 입력, 스칼라 출력*

 입력값이 이진, 즉 0과 1뿐일때의 예시.
 만약 A업체와 B업체에서만 물건을 납품받는다고 가정해보자.

 그러면 x = 0(A), x =1(B)로 입력값이 2개뿐인 이진 입력을 할 수 있게 된다.

 이러면 모델은 다음과 같이 정의된다.

 $p(y|x,\theta) = N(y|\mu_x,\sigma^2_x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y-\mu)^2}{2\sigma^2}}$

 이때 각업체의 이용경험이 한번밖에 없다고 가정을 해보자
 그러면 데이터 셋은 다음과 같이 정의된다.

$D = \{(x_1=0,y_1=15),(x_2=1,y_2=20)\}$

이렇게 되면 MLE를 사용할 경우 $\mu_0 = 15, \mu_1=20$이 되어 버린다.
그러면 이 모델이 납품시간을 예측하면 A업체는 무조건 15분, B업체는  무조건 20분이라는 편향을 가지게 된다.

즉, MLE만을 사용하면 데이터가 부족하거나 노이즈가 있을때, 그 노이즈나, 편향된 값에 과신할 수 있다.
 

#### 사후분포 근사 추론

 사후분포를 항상 계산하는 것은 거의 불가능.
 그래서 사후분포를 근사해서 추론함.

 정확도, 단순성, 속도에서 Trade-Off를 가짐.

Trade-Off : 하나를 선택하면 다른 것은 포기해야하는 상황

 베이지안 머신러닝은, 정확도(신뢰도)를 선택하고 단순성, 속도를 포기함.
 일반적인 DNN보다 복잡하고, 속도가 느림.
 하지만 근사알고리즘을 사용해서 최근에는 상당히 완화가 되었음.



