

기초 내용 [[Bayesian Rule(베이시안 규칙, 정리 기타등등) | 베이즈 규칙]]


### 1. Conjugate Priors(공액 사전)

 사후 분포를 계산할 수 있는 사전확률과 Likelihood를 고려.
  likelihood에 conjugate인 사전분포를 사용하는 경우.

 사전 분포를 $p(\theta) \in F$라 할때 사후 분포 p($\theta|D$) $\in F$다.
  즉 사전분포와 사후 분포가 같은 분포의 형태를 가진다는 뜻이다.
  F는 분포의 가족(지수족)를 의미( ex. 가우시안, 베르누이, 카테고리 등)




### 2. 베타 이항모델


 동전을 N번 던져서 앞면의 확률을 추론할때 $y_n = 1$ 은 n번째 시도가 앞면이었다고 나타내고, $y_n = 0$은 n번째 시도가 뒷면이었다고 나타냄

 이는 $D = \{y_n : n = 1 : N\}$으로 나타내도 D는 모든 데이터를 의미
 이를 $y_n ~ Ber(\theta)$ 로 가정함. 베타 이항모델은 $p(\theta|D)$를 계산하는 방법을 설명함.

#### 베르누이 Likelihood

 데이터는 iid(모두 독립이고 같은분포의 형태를 띈다)라고 가정함.
그래서 베르누이 likelihood는 곱셈으로 정의


$p(D|\theta) = \prod_{n=1}^{N}\theta^{y_n}(1-\theta)^{1-y_n} = \theta^{N_1}(1-\theta)^{N_0}$

$N_1 = \sum^N_{n=1} I(y_n=1)$ 즉, 앞면인 경우의 총합
$N_0 = \sum_{n=1}^{N} I(y_n=0)$ 동전이 뒷면인 경우의 총합
$N = N_1 + N_0$


#### Binomial Likelihood( 이항 Likelihood )

Likelihood : $P(D|\theta) = Bin(y|N,\theta) = \binom{N}{y} \theta^y (1 - \theta)^{N-y}$
$\binom{N}{y}$는 $\theta$와 관련이 없으므로 무시가능


#### 사전(prior)

  베타 이항모델에서의 사전분포는 켤레(conjugate prior)이라 가정
  즉, $p(\theta) \in Beta$로 사전분포가 베타분포의 형태를 띔
$p(\theta) \propto \theta^{\tilde{\alpha} - 1} (1 - \theta)^{\tilde{\beta} - 1} \propto \text{Beta}(\theta | \tilde{\alpha}, \tilde{\beta})$

#### 사후분포

사후분포는 베르누이 Likelihood $\times$ 사전분포 / $p(D)$

즉, 다음과 같이 정의됨 
Posterior =  $\theta^{\tilde{\alpha} - 1} (1 - \theta)^{\tilde{\beta} - 1}\times\theta^{N_1}(1-\theta)^{N_0} = \theta^{\tilde{\alpha} - 1+N_1} (1-\theta)^{N_0+\tilde{\beta} - 1}$
 $\hat\alpha$ = $\tilde\alpha+ N_1$, $\hat\beta = \tilde\beta+N_0$라 가정하면 posterior = $\theta^{\hat\alpha} (1 - \theta)^{\hat{\beta}}$

#### 사후 최빈값(Posterior Mode)

 $\hat{\theta}_\text{MAP} = \arg\max_\theta p(\theta|D) = argmax_\theta\ (log\ p(\theta) + log\ p(D|\theta))$
 

#### 사후 평균(Posterior Mean)

 사후 최빈값은 사후분포의 요약으로 부족할 수 있음
 사전 강도와 MLE의 볼록 결합


 mean = $\int p(\theta|D) d\theta$  = E$[\theta|D]$ = $\frac{\hat\alpha}{\hat\alpha+\hat\beta}$ = $\frac{\hat\alpha}{\hat N}$
 $E[\theta|D] = \frac{\tilde{\alpha} + N_1}{\tilde{\alpha} + N_1 + \tilde{\beta} + N_0} = \frac{\tilde{N} m + N_1}{N + \tilde{N}} = \frac{\tilde{N}}{N + \tilde{N}} m + \frac{N}{N + \tilde{N}} \frac{N_1}{N} = \lambda m + (1 - \lambda) \hat{\theta}_\text{MLE}$

사전 강도가 약할 수록 $\lambda$가 작아져 사후평균이 MLE에 가까워짐


#### 사후분산

 추정치의 불확실성 측정을 위한 분산.
 추정치의 표준오차를 계산하는것.


 se($\theta$) = $\sqrt {V[\theta|D]}$ 

  $V[\theta|D] = \frac{\hat\alpha\hat\beta}{(\hat\alpha+\hat\beta)^2(\hat\alpha+\hat\beta+1)}$ 
  
$V[\theta|D] \approx \frac{N_1 N_0}{N^3} = \frac{\hat{\theta} (1 - \hat{\theta})}{N}$


$\sigma = \sqrt{V[\theta|D]} \approx \sqrt{\frac{\hat\theta(1-\hat\theta)}{N}}$

불확실성(se)은 $\frac{1}{\sqrt{N}}$비율로 감소함. 분산이 $\hat\theta = 0.5$일때 최대가 되고, 0이나 1에 가까울때 최소
1또는 0이면 분산이 0이 되기 때문

#### 사후예측(posterior Predictive)

 훈련된 데이터로 사후 예측을 함. 이때 한쪽에 치우쳐져(과적합되어) 있을 수 있으므로 MAP추정으로 계산.


Plug-in Approximation : 훈련 데이터로 $\theta$를 계산

 이때 과적합이 발생할 수 있음. 
   ex) 동전을 3번 던졌을 때 3번 모두 앞면이 나오면 뒷면이 나올 확률이 무시되는 과적합현상이 발생

 MAP으로 추정하여 사전확률의 강도를 줌(뒷면이 나올수도 있다라는 것을 인식. 과적합 방지)

 fully Bayesian을 사용하여 parameter를 점추정하지 않고, 전체 분포를 사용해 예측
 $\theta$를 점이 아닌 분포 $p(\theta|D)$로 취급하여 평균을 구해 새로운 데이터 값을 예측

$p(y_new|D) = E[\theta|D]$

 즉 fully Bayesian은 예측을 한가지의 값이 아닌 균형이 맞도록 범위(분포)로 정해 과적합을 방지.


#### 베르누이 모델의 사후 예측

  베르누이 모델 같은 경우 사후 예측 분포가 다음과 같음

$p(y=1 | D) = \int_{0}^{1}p(y=1|\theta)p(\theta)|D)d\theta = E[\theta|D] = \frac{\hat\alpha}{\hat\alpha+\hat\beta}$


 add one smoothing을 beta(1,1)을 사용해도 얻을 수 있음.

$p(y =1 |D) = \frac{N_1 + 1}{N_1+N_0 + 2}$


#### 이항 모델의 사후예측


Fully Bayesian
$p(y|D, M) = \int_0^1 \text{Bin}(y|M, \theta) \text{Beta}(\theta | \hat{\alpha}, \hat{\beta}) d\theta = \binom{M}{y} \frac{1}{B(\hat{\alpha}, \hat{\beta})} \int_0^1 \theta^y (1 - \theta)^{M-y} \theta^{\hat{\alpha}-1} (1 - \theta)^{\hat{\beta}-1} d\theta$
 
  $\int_0^1 \theta^{y + \hat{\alpha} - 1} (1 - \theta)^{M - y + \hat{\beta} - 1} d\theta = B(y + \hat{\alpha}, M - y + \hat{\beta})$


$\text{Bb}(x|M, \hat{\alpha}, \hat{\beta}) \triangleq \binom{M}{x} \frac{B(x + \hat{\alpha}, M - x + \hat{\beta})}{B(\hat{\alpha}, \hat{\beta})}$

 Fully Bayesian은 예측을 점이 아닌 분포로 취급
 이 분포의 평균을 사후예측 값으로 사용.
 분포의 평균을 값으로 사용해서 적은 데이터에선 플러그인 근사보다 조금 더 부드러운 예측이 가능

현재 데이터가 Head = 3, tail = 0이고 사전분포가 Beta(2,2)이면 사후 분포는 Beta(3+2,2)로 플러그인 보단 조금 더 부드러움


플러그인 근사

$p(\theta|D) \approx \delta(\theta - \hat{\theta})$
$p(y|D, M) = \int_0^1 \text{Bin}(y|M, \theta) p(\theta|D) d\theta = \text{Bin}(y|M, \hat{\theta})$

플러그인 근사는 간단하지만 불확실성을 무시함. overfit 문제가 있음
플러그인도 MAP로 사전확률을 사용해 예측 하지만 unseen 이벤트에서 현재 나오지 않은 이벤트의 확률이 극단적으로 작다는 단점이 있음. 그 때문에 적은 데이터에선 overfit(과적합) 위험이 큼

현재 데이터가 Head = 3, tail = 0이고 Beta(2,2)이면 

 $\hat\theta_{mle} = \frac{3 + 2 -1}{3+ 2 + 2 -2} = \frac{4}{5} = 0.8$
 

플러그인 근사는 한개의 값을 예측.


데이터가 적을 땐 Fully Bayesian을 많은 때엔 플러그인 근사가 좋음



#### 주변 Likelihood(Marginal Likelihood)

 Marginal Likelihood : 데이터 D가 매개변수없이 사전 분포만으로 나올 확률
 
 모델 M에 대한 주변 Likelihood 또는 증거(evidence)는 다음과 같이 정의됨

$p(D|M) = \int p(\theta|M) p(D|\theta, M) d\theta$

또는 $p(D|h =k) = \int p(D|\theta)p(\theta|h=k)d\theta$

$p(D|\theta)$는 Likelihood
$p(\theta|h=k)$는 조건부 사전확률
적분으로 분포의 평균을 냄

$\theta$를 적분으로 없애서 모델 전체의 증거 역할을 함

 보통의 경우 $\theta$항과는 무관하여 무시됨
 하지만 모델의 비교와 하이퍼 파라미터 튜닝의 핵심도구 역할을 함

 주변 Likelihood를 모델의 데이터 설명도로 사용 : 모델 비교시
 
 주변 Likelihood가 높으면 fit이 좋고 overfit 가능성 존재
   낮으면 underfit 가능성 존재


 Empirical Bayes Prior를 데이터로 튜닝


 ex) 상황: 웹사이트 클릭 D (N=100 trials, 20 clicks)
      Hyperparam 후보: (α,β)=(1,1) uniform vs (2,2) weak fair.
      - p(D|(1,1)) = \binom{100}{20} B(21,81)/B(1,1) ≈ 0.0001 (계산).
	  - p(D|(2,2)) ≈ 0.00008 (작음).
	  P(D|(1,1)) 이 더 높으므로 1,1 하이퍼파라미터 선택



 베르누이에서 Marginal Likelihood


#### 혼합 켤레분포( mixtures of conjugate priors)

 현실에선 사전분포가 단일 형태를 띄지 않는 경우도 존재. (공정하다고 생각하기도 하고, 편향되어 있다고 생각도 할 수 있기 때문)
 그래서 사전분포를 mixture 분포 형태로 혼합하여 표현.


ex)

카지노에서 동전던지기를 할때, 동전이 앞면 뒷면 두가지가 같은 확률로 나온다고 생각도 하지만, 카지노에서 동전에 장난질을 쳐놨다고 생각할 수 있음. 이 경우 하나의 베타 분포로 표현이 불가능.
multimodal을 표현못하기에 혼합 분포 사용

 사전분포 $p(\theta)$ = $0.5\ Beta(20,20)$ + $0.5\ Beta(30,10)$ 

 위에선 동전이 공정하다고 생각하는게 0.5, 앞면이 더 잘나올것이라 생각하는 편향이 0.5라고 가정한 상태

h 를 이산잠재변수라 가정하면 다음과 같이 식이 정의 됨
 h는 현재 몇번쨰 분포와 가중치인지를 가리킴

$p(\theta) = \sum_kp(h=k)p(\theta|h=k)$

 전확률 공식으로 $\sum_kp(h=k,\theta) = p(\theta)$가 성립
 $p(h=k)$는 prior mixing weight(사전 혼합 가중치)

  
후방 혼합

$p(\theta|D) = \sum_kp(h=k|D)p(\theta|D, h=k)$
$p(h=k|D)$는 weight로 후방 weight
$p(\theta|D, h=k)$는 조건부 후방.
$p(D|h=k)$는 k를 위한 주변 Likelihood

사전분포가 저렇게 주어지고 앞면이 20번 뒷면이 10번 나온 상황이면

$P(\theta|D) = 0.346Beta(\theta|20+20,20+10) + 0.654Beta(\theta|30+20, 10+10)$

w1 = 0.5 $\times p(D|h=1)$
w2 = 0.5 $\times p(D|h=2)$

p(D|h=1) => marginal Likelihood를 계산해서 업데이트


사전 혼합 가중치(prior mixing weights):

 초기의 사전 가중치- 현재 데이터셋의 데이터를 보지 못한 상태.
 사전을 p(\theta)라고 가정

 $p(\theta) = \sum_k p(h=k)p(\theta|h=k)$
 
 지금은 현재 아직 데이터셋이 없어서, 현재 예시에선, 조작된 동전과, 평범한 동전의 차이를 각각 0.5씩 설정
 식은 Marginal Likelihood * prior weight

 

 
후방 혼합 가중치(posterior Mixing Weights)

 데이터셋의 데이터를 보고 prior을 업데이트한 weight
 
 marginal Likelihood로 업데이트


 P(D | h = 1) => 현재 공정한 동전이라 가정
 p(D | h = 2) => 조작된 동전이라고 가정


  $p(D | h= 1) =  \binom{30}{20}\frac{B(40,30)}{B(20,20)}$

  $p(D | h= 2) = \binom{30}{20}\frac{B(50,20}{B(30,10)}$

  현재 입력 데이터에, 공정한 사전 분포를 더해 분포를 계산. 분모로 공정한 사전분포를 사용해, 가설인 사전분포가 잘 맞는지 확인
  h=1은, 공정하다고 생각한 가설, h=2는 조작된 동전이라고 생각한 가설


 이 Marginal Likelihood에 가설의 사전 weight  0.5를 더해 가설의 비중을 업데이트

 k=1 => $0.04365 \times 0.5 = 0.021825$
 k=2 => $0.08259 \times 0.5 = 0.041295$

 그후 이 가중치를 정규화.
 정규화할 분모는 k=1과 k=2의 가설 weight의 합 => 0.021825 + 0.041295 = 0.06312

후방가중치 $p(h=1 | D)$ 와 $p(h = 2 | D)$로 업데이트

 $p(h=1|D)$ = 0.021825/0.06312 = 0.346
 $p(h=2|D)$ = 0.041295/0.06312 = 0.654


결과적으로 후방분포 $p(\theta|D)$는 저 후방분포와 후방가중치의 곱들의 합으로 이루어짐

$p(\theta|D) = 0.346Beta(40,30) + 0.654 Beta(50,20)$

이 후방분포는 동전이 앞면일 확률 자체의 확률 분포. 즉, 예상값, 출력값들의 확률분포





 2진 확률을 k진 확률로 일반화

 베르누이 -> 디리클레 다항 모델

ex) 동전(2진) -> 주사위(6진)


 하나의 시행에서 K개의 결과중 1개만을 선택하는 확률 모델


디리클레 다항모델의 Likelihood:

 $Y~Cat(\theta)$의 Likelihood

 $p(D|\theta) = \Pi_{n=1}^{N}Cat(y_n|\theta) = \Pi_{n=1}^N\Pi_{c=1}^C\theta_c^{I(y_n=c)} = \Pi_{c=1}^C \theta_c^{N_c}$

 $N_c = \sum_n I(y_n = c)$로 데이터에서 카테고리 C가 나타난 횟수.
 
  결국 하나의 $\theta_c$에서만 지시함수가 1이므로 $\Pi_{c=1}^C\theta_c^{I(y_n=c)}$ 에서 \theta_{yc}만 남음
  그래서 축약하면 전체 Likelihood는 다음과 같아짐

 $p(D|\theta) = \Pi_{n=1}^N\theta_{yc}$


 결과적으로 $p(D|\theta) = \theta_1^{N_1}\theta_2^{N_2}\...\theta_C^{N_C}$
 


 $N_c$는 데이터에서 카테고리 c가 나타난 횟수
 ex) 주사위를 100번 던져서 1이 20번, 2가 30번이면 $N_1$ = 20, $N_2$ = 30이 됨

Prior

 이 사전의 합. $\sum_k \theta_k = 1, \theta_k >= 0$
 카테고리 분포의 conjugate 사전 분포는 디리클레 분포. 디리클레 분포는 다변량 베타 분포

 디리클레 분포는 확률 심플렉스(probablity simplex)에서 정의됨.
 

 -  확률 심플렉스(probablity simplex)
   $\mathbb{S}_K = \left\{ \theta : 0 \leq \theta_k \leq 1, \sum \theta_k = 1 \right\}$

  K = 2: 선분 (0,1) ~ (1,0) 사이
  K = 3: 정삼각형 (모서리:(1,0,0),(0,1,0),(0,0,1))
  K = 4는 정사면체 내부

   모든 확률 벡터 $\theta$는 심플렉스 위에만 존재


 

Posterior
 MAP 추정 사용

 
 사후분포의 parameter 업테이트 규직.


  사후분포는 사전 가상 관측수 + 실제 관측수로 구함

 사후 평균 :

 $\hat\theta_k = \frac{\hat\alpha_k}{\sum_{k'}\hat\alpha_{k'}} = \frac{\tilde\alpha+N_k}{\sum_{k'}(\tilde\alpha_{k'}(\tilde\alpha_{k'}+N_{k'})}$

 N : 실제 데이터 수
 $\alpha_0$: 사전의 강도
 $\frac{N_k}{N}$ : 데이터 빈도 (얼마나 그 데이터가 나타났는지)
 $\frac{\tilde\alpha_k}{\alpha_0}$ : 사전 평균


 스무딩 효과 :
   데이터 N이 많을수록 -> 데이터 비중 올라감 -> MLE에 수렴
   데이터 N이 적으면 -> 사전비중이 올라감 -> 과적합 방지

 - 스무딩 효과 -> 데이터가 없는 경우 0이 아닌 작은 확률을 부여해 추정값이 극단값에 치우치는 것을 방지하는 기법
       사전에 모든 카테고리가 조금씩 관측된 것처럼 가정


 
3. 사후 최빈값 (Posterior Mode = MAP)
$$\tilde{\theta}_k = \frac{\hat{\alpha}_k - 1}{\sum_{k'} (\hat{\alpha}_{k'} - 1)} = \frac{\tilde{\alpha}_k + N_k - 1}{\sum (\tilde{\alpha}_{k'} + N_{k'} - 1)}$$

디리클레 분포의 최빈값(mode)은 항상 $ \alpha_k - 1 $ 기반.
$ \tilde{\alpha}_k = 1 $이면:
$$\tilde{\theta}_k = \frac{N_k}{N} \quad \Rightarrow \text{MAP = MLE}$$



4. 균등 사전일 때: MAP = MLE
$$\tilde{\alpha}_k = 1 \quad \forall k \quad \Rightarrow \quad \text{Dir}(1,1,\dots,1)$$

이 경우:
$$\hat{\alpha}_k = 1 + N_k, \quad \sum \hat{\alpha}_{k'} = K + N$$
$$\text{MAP} = \frac{N_k}{N} = \text{MLE}$$


사후 예측 분포

 - 과거 데이터를 보고 새로운 데이터가 나올 확률을 예측하는 분포

 사전 $\theta$를 고정된 값이 아닌 분포로 생각.
 
  사후 예측 분포 = "훈련 데이터 + 가상으로 관측한 주사위"에서 나올 확률




Marginal Likelihood

 $\theta$ 없이 데이터만으로 계산되는 확률


### 가우시안 가우시안 모델

사전 사후 Likelihood가 모두 가우시안 모델인 켤레 사후 분포
보통은 켤레 사전분포를 따라 사전분포와 사후 분포만 같은데, 가우시안 가우시안은 Likelihood까지 가우시안 모델.

### 켤레 사전분포 너머

 지금까지의 켤레사전분포는 지수족에 속함.
 하지만 현실의 대부분의 모델에선 켤레인 사전분포가 거의 없음.


	모델     사전
 로지스틱        시그모이드    -> 지수족 아님
 포아송 회귀     로그링크      -> 비선형
 혼합 가우시안   혼합           -> 복잡
 신경망             없음

  
#### 비정보적 사전 분포

   데이터에 대한 지식이 거의 없거나 아예 없을때 사용하는 사전분포
   데이터가 스스로 말하게 하라는 원칙을 따름
	   사전이 추정에 거의 영향을 주지 않도록 하는 사전분포
	   아무것도 모르므로 Parameter에 대해 가능한 중립적인 입장을 표현하려는 시도

 비정보적 사전분포를 정의하는 고유한 방법은 없음.



#### 계층적 사전분포(Hierarchical priors)

   베이지안 모델은 Parameter에 대한 사전분포를 지정해야함

  사전분포의 매개변수 -> HyperParameter 즉, 사전분포는 우리가 정해야하는 HyperParameter.
  
  사전분포의 parameter를 $\xi$(싸이)라고 함.
  즉, $\xi$로 $\theta$를 추론하고, $\theta$로 $D$를 추론하는 다단계형식의 계층적 모델.

 계층적 베이지안 모델 또는 다단계 모델이라고 정의함.

#### 경험적 사전분포(Empirical priors)

 MAP, 즉, Posterior 추론의 계산이 편리해지는 근사방법을 설명.
 간단히 말해 사전분포의 HyperParameter를 근사, 추정하는것.

 이때 Type Ⅱ  maximum Likelihood를 사용해 HyperParameter를 추정.

##### Type Ⅱ  maximum Likelihood
 
   데이터를 사용해 베이지안 모델의 하이퍼파라미터를 최적화.
   일반 MLE와 달리 Parameter가 아닌 사전분포의 Hyperparameter를 찾는데 중점을 둠

  데이터 D가 가장 잘 나타나는 하이퍼파라미터를 찾는것.
  


### 베이지안 신뢰 구간(Credible Intervals)

  신뢰구간 : 고차원의 분포를 요약하는데 사용하는데 사용.
   데이터를 봤을때 Parameter가 특정 구간안에 있을 확률. 


 머신러닝에서 보면, Parameter와 예측값도 분포를 가진다고 생각해서 사용함.

 Weight가 특정 구간에 있을 확률이 95% , 입력값이 x일때 예측값이 어느 구간에 있을 확률이 95%일때 여기서 말한 특정 구간이 신뢰구간.

 머신러닝에선 예측을 얼마나 확신하는지를 표현할때 사용.

 신뢰구간이 

 



### 베이즈 머신러닝


